{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "iGpn-FAMRIq2",
        "outputId": "ae5e476d-a81e-40c8-cfac-69d8c779758d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'SimpleRNN' object has no attribute 'index_to_word'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f50480e206f8>\u001b[0m in \u001b[0;36m<cell line: 43>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# Convert the numeric predictions to a sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mpredicted_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Predicted Sentence:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f50480e206f8>\u001b[0m in \u001b[0;36mpredict_sentence\u001b[0;34m(self, y_pred)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Convert output to actual words by choosing the word with the highest score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get index of the predicted word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Convert indices to words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-f50480e206f8>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Convert output to actual words by choosing the word with the highest score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Get index of the predicted word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_to_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Convert indices to words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'SimpleRNN' object has no attribute 'index_to_word'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, vocab_size):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size  # Define vocabulary size for word predictions\n",
        "\n",
        "        # Weights initialization\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01  # Output for each word in vocab\n",
        "\n",
        "        # Biases initialization\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = np.zeros((self.hidden_size, 1))\n",
        "        self.h_states = [h]\n",
        "\n",
        "        for x in inputs:\n",
        "            x = x.reshape(-1, 1)\n",
        "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
        "            self.h_states.append(h)\n",
        "\n",
        "        y = np.dot(self.Why, h) + self.by  # Predict scores for each word in the vocabulary\n",
        "        return y\n",
        "\n",
        "    def predict_sentence(self, y_pred):\n",
        "        # Convert output to actual words by choosing the word with the highest score\n",
        "        word_index = np.argmax(y_pred, axis=0)  # Get index of the predicted word\n",
        "        sentence = [self.index_to_word[i] for i in word_index # Convert indices to words\n",
        "        return \" \".join(sentence)\n",
        "\n",
        "# Initialize with vocabulary size, input_size can be embedding size for word vectors\n",
        "rnn = SimpleRNN(input_size=3, hidden_size=5, output_size=2, vocab_size=10)\n",
        "\n",
        "# Assuming `inputs` is encoded text data and `target` contains word indices for the sentence\n",
        "inputs = np.random.rand(10, 3)\n",
        "y_pred = rnn.forward(inputs)\n",
        "\n",
        "# Convert the numeric predictions to a sentence\n",
        "predicted_sentence = rnn.predict_sentence(y_pred)\n",
        "print(\"Predicted Sentence:\", predicted_sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class SimpleRNN:\n",
        "    def __init__(self, input_size, hidden_size, output_size, vocab_size, index_to_word):\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size  # Define vocabulary size for word predictions\n",
        "        self.index_to_word = index_to_word  # Dictionary mapping indices to words\n",
        "\n",
        "        # Weights initialization\n",
        "        self.Wxh = np.random.randn(hidden_size, input_size) * 0.01\n",
        "        self.Whh = np.random.randn(hidden_size, hidden_size) * 0.01\n",
        "        self.Why = np.random.randn(vocab_size, hidden_size) * 0.01  # Output for each word in vocab\n",
        "\n",
        "        # Biases initialization\n",
        "        self.bh = np.zeros((hidden_size, 1))\n",
        "        self.by = np.zeros((vocab_size, 1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = np.zeros((self.hidden_size, 1))\n",
        "        self.h_states = [h]\n",
        "\n",
        "        for x in inputs:\n",
        "            x = x.reshape(-1, 1)\n",
        "            h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, h) + self.bh)\n",
        "            self.h_states.append(h)\n",
        "\n",
        "        y = np.dot(self.Why, h) + self.by  # Predict scores for each word in the vocabulary\n",
        "        return y\n",
        "\n",
        "    def predict_sentence(self, y_pred):\n",
        "        # Convert output to actual words by choosing the word with the highest score\n",
        "        word_index = np.argmax(y_pred, axis=0)  # Get index of the predicted word\n",
        "        sentence = [self.index_to_word[i] for i in word_index.flatten()]  # Convert indices to words\n",
        "        return \" \".join(sentence)\n",
        "\n",
        "# Define vocabulary and index-to-word mapping\n",
        "index_to_word = {\n",
        "    0: \"It\",\n",
        "    1: \"is\",\n",
        "    2: \"a\",\n",
        "    3: \"pleasant\",\n",
        "    4: \"day\"\n",
        "}\n",
        "vocab_size = len(index_to_word)\n",
        "\n",
        "# Initialize RNN with the index_to_word mapping\n",
        "rnn = SimpleRNN(input_size=3, hidden_size=5, output_size=2, vocab_size=vocab_size, index_to_word=index_to_word)\n",
        "\n",
        "# Assuming `inputs` is encoded text data and `target` contains word indices for the sentence\n",
        "inputs = np.random.rand(10, 3)\n",
        "y_pred = rnn.forward(inputs)\n",
        "\n",
        "# Convert the numeric predictions to a sentence\n",
        "predicted_sentence = rnn.predict_sentence(y_pred)\n",
        "print(\"Predicted Sentence:\", predicted_sentence)\n"
      ],
      "metadata": {
        "id": "45_UZs16SkY6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cffc22e6-4416-4cde-cbe6-18976e5832c0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Sentence: day\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lht9P2rQ_O_A"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}